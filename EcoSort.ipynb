{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05iOqtW2xMo3","executionInfo":{"status":"ok","timestamp":1712379505267,"user_tz":-330,"elapsed":33072,"user":{"displayName":"SAMEERA JATHAR","userId":"07970661253037508957"}},"outputId":"5b8c7b4b-b1f4-4b2f-ba93-e75bd27a4dc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q32Rl0UZh9Lj"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from keras.layers import Activation, Dropout, Flatten, Dense\n","from keras import backend as K\n","from keras.preprocessing import image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K9tf5CsiRc9","executionInfo":{"status":"ok","timestamp":1712379519965,"user_tz":-330,"elapsed":4129,"user":{"displayName":"SAMEERA JATHAR","userId":"07970661253037508957"}},"outputId":"075fd712-06bc-45b8-aaa2-96fb15c37b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 805 images belonging to 9 classes.\n","Found 225 images belonging to 9 classes.\n"]}],"source":["img_width, img_height = 224, 224\n","\n","train_data_dir = '/content/drive/MyDrive/EcoSort (1)/train_data (1)'\n","validation_data_dir = '/content/drive/MyDrive/EcoSort (1)/test_data (1)'\n","nb_train_samples = 850\n","nb_validation_samples = 225\n","epochs = 5\n","batch_size = 32\n","\n","if K.image_data_format() == 'channels_first':\n","    input_shape = (3, img_width, img_height)\n","else:\n","    input_shape = (img_width, img_height, 3)\n","\n","\n","train_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","test_generator = test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9g2WbMrim-q"},"outputs":[],"source":["from numpy.random import seed\n","seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXElcZ6Hit_c"},"outputs":[],"source":["import tensorflow\n","tensorflow.random.set_seed(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEDBnuwtiwrh","executionInfo":{"status":"ok","timestamp":1712379528751,"user_tz":-330,"elapsed":2028,"user":{"displayName":"SAMEERA JATHAR","userId":"07970661253037508957"}},"outputId":"609cea02-4083-4210-ccbe-0563d959e1ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80134624/80134624 [==============================] - 0s 0us/step\n"]}],"source":["from tensorflow.keras.applications.vgg19 import VGG19\n","\n","base_model = VGG19(input_shape = (224, 224, 3), # Shape of our images\n","include_top = False, # Leave out the last fully connected layer\n","weights = 'imagenet')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzW2EieSi1qp","executionInfo":{"status":"ok","timestamp":1712379529598,"user_tz":-330,"elapsed":855,"user":{"displayName":"SAMEERA JATHAR","userId":"07970661253037508957"}},"outputId":"d603d03a-5a08-4472-b001-89986426bacc"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dropout (Dropout)           (None, 25088)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 9)                 225801    \n","                                                                 \n","=================================================================\n","Total params: 20250185 (77.25 MB)\n","Trainable params: 20250185 (77.25 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import layers\n","from keras.models import load_model\n","from keras.layers import Lambda\n","import tensorflow as tf# Flatten the output layer to 1 dimension\n","x = layers.Flatten()(base_model.output)\n","\n","# Add a fully connected layer with 512 hidden units and ReLU activation\n","y = layers.Dense(224, activation='relu')(x)\n","\n","# Add a dropout rate of 0.5\n","x = layers.Dropout(0.5)(x)\n","\n","# Add a final sigmoid layer for classification\n","x = layers.Dense(9, activation='softmax')(x)\n","\n","model = tf.keras.models.Model(base_model.input, x)\n","\n","model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'categorical_crossentropy',metrics = ['acc'])\n","model.summary()\n","for layer in base_model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWQJVZigjAB6","outputId":"d7dc7083-eb31-4deb-9e25-c8edf62e5a99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"," 4/20 [=====>........................] - ETA: 5:01 - loss: 3.8058 - accuracy: 0.1188"]}],"source":["model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics =[\"accuracy\"])\n","vgghist = model.fit(train_generator, validation_data = test_generator, steps_per_epoch = 20, epochs = 10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUJH2bcRjDET"},"outputs":[],"source":["from tensorflow.keras.preprocessing import image\n","output_class = [\"batteries\", \"clothes\", \"e-waste\", \"glass\", \"light blubs\", \"metal\", \"organic\", \"paper\", \"plastic\"]\n","def waste_prediction(new_image):\n","  test_image = image.load_img(new_image, target_size = (224,224))\n","  plt.axis(\"off\")\n","  plt.imshow(test_image)\n","  plt.show()\n","\n","  test_image = image.img_to_array(test_image) / 255\n","  test_image = np.expand_dims(test_image, axis=0)\n","\n","  predicted_array = model.predict(test_image)\n","  predicted_value = output_class[np.argmax(predicted_array)]\n","  predicted_accuracy = round(np.max(predicted_array) * 100, 2)\n","\n","  print(\"Your waste material is \", predicted_value, \" with \", predicted_accuracy, \" % accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmssqpnxjNNg"},"outputs":[],"source":["plt.title(\"Accuracy\")\n","plt.plot(vgghist.history[\"accuracy\"])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S-gzhbOjNQq"},"outputs":[],"source":["plt.title(\"Loss\")\n","plt.plot(vgghist.history[\"loss\"])\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRgxqme3XEFZ"},"outputs":[],"source":["def plot_confusion_matrix(cm, target_names, cmap=None):\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title('Confusion matrix')\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}%; misclass={:0.4f}%'.format(accuracy, misclass))\n","    plt.show()\n","\n","plt.figure(figsize=(10, 10))\n","true = []\n","predictions = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8nZ6l6HXEI0"},"outputs":[],"source":["url = \"/content/drive/MyDrive/EcoSort (1)/test_data (1)/clothes (1)/t-shirt-waste-clothes-250x250 (1).jpg\"\n","image = tf.keras.utils.get_file(\"Cotton-T-Shirt-Waste-Cloth-w410 (1)\", origin=url)\n","\n","img = tf.keras.preprocessing.image.load_img(image, target_size=(224, 224))\n","img_array = tf.keras.preprocessing.image.img_to_array(img)\n","img_array = tf.expand_dims(img_array, 0)\n","\n","predictions = model.predict(img_array)\n","#score = tf.nn.softmax(predictions[0])\n","\n","plt.imshow(img)\n","# print(predictions)\n","# print(\"Prediction: \" + str(classes[np.argmax(predictions)]))\n","print(predictions[0]*100, \"\\n\", classes)\n","print(\"Prediction: \", classes[np.argmax(predictions)], f\"{predictions[0][np.argmax(predictions)]*100}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7RdbWFtYTXE"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}